# -*- coding: utf-8 -*-
"""Programming2_Virag_Vörös.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YZ8Bu_ndEukPBnTLaIGtOPSNaUiCdJGn

# Algorithms for Uncertainty Quantification
## Programming assignment 2, Summer term 2022
### Virag Vörös
"""

!pip install chaospy

import numpy as np
import chaospy as cp
from scipy.integrate import odeint
from matplotlib.pyplot import *
import time

"""# Assignment 1.1

"""

# to perform barycentric interpolation, we'll first compute the barycentric weights
def compute_barycentric_weights(grid):
    size    = len(grid)
    w       = np.ones(size)

    for j in range(1, size):
        for k in range(j):
            diff = grid[k] - grid[j]

            w[k] *= diff
            w[j] *= -diff

    for j in range(size):
        w[j] = 1./w[j]

    return w

# rewrite Lagrange interpolation in the first barycentric form
def barycentric_interp(eval_point, grid, weights, func_eval):
    interp_size = len(func_eval)
    L_G         = 1.
    res         = 0.

    for i in range(interp_size):
        L_G   *= (eval_point - grid[i])

    for i in range(interp_size):
        if abs(eval_point - grid[i]) < 1e-10:
            res = func_eval[i]
            L_G    = 1.0
            break
        else:
            res += (weights[i]*func_eval[i])/(eval_point - grid[i])

    res *= L_G 

    return res

# to use the odeint function, we need to transform the second order differential equation
# into a system of two linear equations
def model(w, t, p):
	x1, x2 		= w
	c, k, f, w 	= p

	f = [x2, f*np.cos(w*t) - k*x1 - c*x2]

	return f

# discretize the oscillator using the odeint function
def discretize_oscillator_odeint(model, atol, rtol, init_cond, args, t, t_interest):
	sol = odeint(model, init_cond, t, args=(args,), atol=atol, rtol=rtol)

	return sol[t_interest, 0]

if __name__ == '__main__':
    # relative and absolute tolerances for the ode int solver
    atol = 1e-10
    rtol = 1e-10

    # parameters setup as specified in the assignement
    c   = 0.5
    k   = 2.0
    f   = 0.5
    y0  = 0.5
    y1  = 0.

    # w is no longer deterministic
    w_left      = 0.95
    w_right     = 1.05
    stat_ref    = [-0.43893703, 0.00019678]

    # create uniform distribution object and set fixed seed

    distr_w = cp.Uniform(w_left, w_right)
    np.random.seed(1234)

    # no of samples from Monte Carlo sampling
    no_samples_vec = [10, 100, 1000, 10000]
    no_grid_points_vec = [2, 5, 10, 20]

    # time domain setup
    t_max       = 10.
    dt          = 0.01
    grid_size   = int(t_max/dt) + 1
    t           = np.array([i*dt for i in range(grid_size)])
    t_interest  = -1

    # create test function
    no_exact = 150
    exact_grid  = np.linspace(w_left, w_right, no_exact)

    # initial conditions setup
    init_cond   = y0, y1

    # create vectors to contain the expectations and variances and runtimes
    err_exp_mcs = np.zeros(len(no_samples_vec))
    err_var_mcs = np.zeros(len(no_samples_vec))

    err_exp_lagrange = np.zeros((len(no_grid_points_vec), len(no_samples_vec)))
    err_var_lagrange = np.zeros((len(no_grid_points_vec), len(no_samples_vec)))

    lagrange_time = np.zeros((len(no_grid_points_vec), len(no_samples_vec)))
    mc_time = np.zeros(len(no_samples_vec))

    # reference values
    E_ref = -0.43893703
    V_ref = 0.00019678
    
    # compute relative error
    relative_err = lambda approx, real: abs(1. - approx/real)

    # perform Monte Carlo sampling
    for j, no_grid_points in enumerate(no_grid_points_vec):
        # a) Create the interpolant and evaluate the integral on the lagrange interpolant using MC
        # generate Chebyshev grid
        cheb_grid = np.array([0.5*(w_left+w_right) + 0.5*(w_right-w_left)*np.cos((2*i - 1)/(2.*no_grid_points) * np.pi) for i in range(1, no_grid_points+1)])

        # evaluate function at grid points, compute barycentric weights        
        start = time.time()
        weights = compute_barycentric_weights(cheb_grid)
        func_eval = np.zeros(no_grid_points)
        for n, w in enumerate(cheb_grid):
                args                = c, k, f, w
                func_eval[n]   = discretize_oscillator_odeint(model, atol, rtol, init_cond, args, t, t_interest)
        end = time.time()
        lagrange_time[j, :] += end-start

        for i, no_samples in enumerate(no_samples_vec):
            np.random.seed(1)
            samples = distr_w.sample(size=no_samples)

            if j == 0:
              # evaluate the integral directly using MC
              start = time.time()
              results_dir_mc = np.zeros(no_samples)
              for m, w in enumerate(samples):
                args = c, k, f, w
                results_dir_mc[m] = discretize_oscillator_odeint(model, atol, rtol, init_cond, args, t, t_interest)

              # compute expectation and variance  
              err_exp_mcs[i] = relative_err(np.mean(results_dir_mc), E_ref)
              err_var_mcs[i] = relative_err(np.var(results_dir_mc, ddof=1), V_ref)

              # measure time
              end = time.time()
              mc_time[i] = end-start

            start = time.time()
            # perform MC sampling of Lagrange interpolant
            results_lagrange = np.array([barycentric_interp(sample, cheb_grid, weights, func_eval) for sample in samples])

            # compute expectation and variance
            err_exp_lagrange[j, i] = relative_err(np.mean(results_lagrange), E_ref)
            err_var_lagrange[j, i] = relative_err(np.var(results_lagrange, ddof=1), V_ref)

            # measure time
            end = time.time()
            lagrange_time[j, i] += end-start

    color_vec = ['red', 'blue', 'yellow', 'cyan']

    figure()
    for l, no_grid_points in enumerate(no_grid_points_vec):
      plot(no_samples_vec, err_exp_lagrange[:, l], '-', color = color_vec[l], label = 'Lagrange with ' + str(no_grid_points) + ' points')
    plot(no_samples_vec, err_exp_mcs, 'k-', label='direct MC sampling')
    xlabel('no. of MC samples')
    ylabel('relative error of expectation')
    xscale('log')
    yscale('log')
    legend()

    figure()
    for p, no_grid_points in enumerate(no_grid_points_vec):
      plot(no_samples_vec, err_var_lagrange[:, p], '-', color = color_vec[p], label = 'Lagrange with ' + str(no_grid_points) + ' points')
    plot(no_samples_vec, err_var_mcs, 'k-', label='direct MC sampling')
    xlabel('no. of MC samples')
    ylabel('relative error of variance')
    xscale('log')
    yscale('log')
    legend()

    print("Mean error")
    print("Lagrange")
    print(err_exp_lagrange)
    print("Monte Carlo")
    print(err_exp_mcs)
    print("----------------------------------------")
    print("Variance error")
    print("Lagrange")
    print(err_var_lagrange)
    print("Monte Carlo")
    print(err_var_mcs)
    print("----------------------------------------")
    print("Time")
    print("Lagrange")
    print(lagrange_time)
    print("Monte Carlo")
    print(mc_time)
    show()

"""It can be seen that by employing a surrogate model based on Lagrange polynomials, the relative error can be reduced to the same level as with direct MC sampling already for 5 nodes and 100 MC samples, but for a fraction of the computational time. With more nodes the decrease in the relative error is even more severe for a lower number of samples than with direct MC, while still being a lot cheaper.

This means that for more complicated models than this simple toy model, where evaluations are even costlier, a surrogate model can vastly reduce the time it takes to arrive at reasonable error values.

However it should also be noted that low quality surrogate models, e.g. the Lagrange polynomial model with 2 nodes, never achieve the desired order of magnitude for the error and plateau at relatively high values.

# Assignment 2.1
$$\newcommand{\ket}[1]{\left|{#1}\right\rangle}$$
$$\newcommand{\bra}[1]{\left\langle{#1}\right|}$$
$$\newcommand{\braket}[2]{\left\langle{#1}\middle|{#2}\right\rangle}$$
If $X$ is distributed with density $\rho$ then

$\mathbb{E}(\phi_i(X)\cdot\phi_j(X)) = \int \phi_i(x)\phi_j(x)\rho(x) dx = \braket{\phi_i(x)}{\phi_j(x)}_\rho$.

Therefore $\phi_i$ and $\phi_j$ are orthogonal if and only if

$\mathbb{E}(\phi_i(X)\cdot\phi_j(X)) = \gamma_i \delta_{ij}$. ∎

# Assigment 2.2
"""

if __name__ == '__main__':
	np.set_printoptions(precision=7)
	np.set_printoptions(suppress=True)
	# define the two distributions
	unif_distr = cp.Uniform(-1, 1)
	norm_distr = cp.Normal(10, 1)

	# degrees of the polynomials
	N = [8]

	# generate orthogonal polynomials for all N's
	for i, n in enumerate(N):
		
		# employ the three terms recursion scheme using chaospy to generate orthonormal polynomials w.r.t. the two distributions
		poly_unif = cp.expansion.stieltjes(n, unif_distr, normed=True)
		poly_norm = cp.expansion.stieltjes(n, norm_distr, normed=True)

		sol_vec_unif = np.zeros((n, n))
		sol_vec_norm = np.zeros((n, n))

		# compute <\phi_j(x), \phi_k(x)>_\rho, i.e. E[\phi_j(x) \phi_k(x)]
		for j in range(n):
			for k in range(n):
				sol_unif = cp.E(poly_unif[j]*poly_unif[k], unif_distr)
				sol_norm = cp.E(poly_norm[j]*poly_norm[k], norm_distr)

				sol_vec_unif[j, k] = sol_unif
				sol_vec_norm[j, k] = sol_norm

	# print result for specific n
	print('Orthogonality of ith and jth polynomial of degree', n)
	print(sol_vec_unif)
	print('Normal Distribution\n')
	print(sol_vec_norm)

"""# Assignment 3.1
Proof.
$\mathbb{E}[f(t,\omega)] ≈ \mathbb{E}[f^N(t,\omega)] = \mathbb{E}\left[ \sum_{n=0}^{N-1} \hat{f}_n(t) \phi_n(\omega) \right]$.

Due to linearity of the expectation value we can write

$\mathbb{E}\left[ \sum_{n=0}^{N-1} \hat{f}_n(t) \phi_n(\omega) \right] = \sum_{n=0}^{N-1} \hat{f}_n(t) \mathbb{E}[\phi_n(\omega)]$.

By multiplying with 1 we get

$\sum_{n=0}^{N-1} \hat{f}_n(t) \mathbb{E}[\phi_n(\omega)\cdot 1] = \sum_{n=0}^{N-1} \hat{f}_n(t) \mathbb{E}[\phi_n(\omega)\cdot \phi_0(\omega)]$,

since $\phi_0(\omega) = 1$.

By employing the orthonormality of the polynomials $\phi_i$ we get that

$\sum_{n=0}^{N-1} \hat{f}_n(t) \mathbb{E}[\phi_n(\omega)\cdot \phi_0(\omega)] = \sum_{n=0}^{N-1} \hat{f}_n(t) \delta_{n0}$.

Since $\delta_{n0}$ is only non-zero for $n=0$, we get that

$\sum_{n=0}^{N-1} \hat{f}_n(t) \delta_{n0} = \hat{f}_0(t)$ ∎

Proof.
$Var[f(t,ω)] = \mathbb{E}[(f(t,ω) - \mathbb{E}[f(t,ω)])^2] ≈ \mathbb{E}[(f^N(t,ω) - \hat{f}_0)^2]$

by using the result of the above proof.

$\mathbb{E}[(f^N(t,ω) - \hat{f}_0(t))^2] = \mathbb{E}\left[ \left( \sum_{n=0}^{N-1} \hat{f}_n(t) \phi_n(\omega) - \hat{f}_0(t) \right)^2 \right] = \mathbb{E}\left[ \left( \sum_{n=1}^{N-1} \hat{f}_n(t) \phi_n(\omega) \right)^2 \right]$.

Since a squared sum is equal to a sum of squares and the expectation value is linear we can write

$\mathbb{E}\left[ \left( \sum_{n=1}^{N-1} \hat{f}_n(t) \phi_n(\omega) \right)^2 \right] = \sum_{n=1}^{N-1} \hat{f}^2_n(t) \mathbb{E}[\phi^2_n(\omega)]$.

Since $\mathbb{E}[\phi^2_n(\omega)] = \delta_{nn} = 1$ we get

$\sum_{n=1}^{N-1} \hat{f}^2_n(t) \mathbb{E}[\phi^2_n(\omega)] = \sum_{n=1}^{N-1} \hat{f}^2_n(t)$ ∎

# Assignment 4.1
"""

# to use the odeint function, we need to transform the second order differential equation
# into a system of two linear equations
def model(w, t, p):
	x1, x2 		= w
	c, k, f, w 	= p

	f = [x2, f*np.cos(w*t) - k*x1 - c*x2]

	return f

# discretize the oscillator using the odeint function
def discretize_oscillator_odeint(model, atol, rtol, init_cond, args, t, t_interest):
	sol = odeint(model, init_cond, t, args=(args,), atol=atol, rtol=rtol)

	return sol[t_interest, 0]


if __name__ == '__main__':
    ### deterministic setup ###

    # relative and absolute tolerances for the ode int solver
    atol = 1e-10
    rtol = 1e-10

    # parameters setup as specified in the assignement
    c   = 0.5
    k   = 2.0
    f   = 0.5
    y0  = 0.5
    y1  = 0.

    # time domain setup
    t_max       = 10.
    dt          = 0.01
    grid_size   = int(t_max/dt) + 1
    t           = np.array([i*dt for i in range(grid_size)])
    t_interest  = -1

    # initial conditions setup
    init_cond   = y0, y1

    ### stochastic setup ####
    # w is no longer deterministic
    w_left      = 0.95
    w_right     = 1.05

    # create uniform distribution object
    distr_w = cp.Uniform(w_left, w_right)

    # the truncation order of the polynomial chaos expansion approximation
    N = [1, 2, 3, 4, 5, 6]
    # the quadrature degree of the scheme used to computed the expansion coefficients
    K = [1, 2, 3, 4, 5, 6]

    # vector to save the statistics
    exp_m = np.zeros(len(N))
    var_m = np.zeros(len(N))

    exp_cp = np.zeros(len(N))
    var_cp = np.zeros(len(N))

    # vector to savec number of samples
    num_samples_vec = np.zeros(len(N))

    # perform polynomial chaos approximation + the pseudo-spectral
    for h in range(len(N)):
        # create N[h] orthogonal polynomials using chaospy
        poly            = cp.expansion.stieltjes(N[h], distr_w, normed=True)
        # create K[h] quadrature nodes using chaospy
        nodes, weights  = cp.generate_quadrature(K[h], distr_w, rule='g')
        num_samples_vec[h] = len(nodes[0])

        # evaluate function
        func_eval = np.zeros(len(nodes[0]))
        for id, w in enumerate(nodes[0]):
            param = c, k, f, w
            func_eval[id] = discretize_oscillator_odeint(model, atol, rtol, init_cond, param, t, t_interest)

        # perform polynomial chaos approximation + the pseudo-spectral approach manually
        gpc_manu = np.zeros(len(poly))
        for i in range(len(poly)):
            for j in range(len(nodes[0])):
                gpc_manu[i] += func_eval[j] * poly[i](nodes[0, j]) * weights[j]
        exp_m[h] = gpc_manu[0]
        var_m[h] = np.sum([gpc_manu[l]**2 for l in range(1, len(poly))])

        # perform polynomial chaos approximation + the pseudo-spectral approach using chaospy
        gpc_matrix = cp.fit_quadrature(poly, nodes, weights, func_eval)

        exp_cp[h] = cp.E(gpc_matrix, distr_w)
        var_cp[h] = cp.Var(gpc_matrix, distr_w)
        
    
    print('MEAN')
    print("K | N | Manual \t\t\t| ChaosPy")
    for h in range(len(N)):
        print(K[h], '|', N[h], '|', "{a:1.12f}".format(a=exp_m[h]), '\t|', "{a:1.12f}".format(a=exp_cp[h]))

    print('VARIANCE')
    print("K | N | Manual \t\t| ChaosPy")
    for h in range(len(N)):
        print(K[h], '|', N[h], '|', "{a:1.12f}".format(a=var_m[h]), '\t|', "{a:1.12f}".format(a=var_cp[h]))