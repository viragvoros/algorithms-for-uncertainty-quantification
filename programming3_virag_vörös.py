# -*- coding: utf-8 -*-
"""Programming3_Virag_Vörös.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Xt0BY88tCBgFrKmT_FCWeTRME9J5zaQu

# Algorithms for Uncertainty Quantification
## Programming assignment 3, Summer term 2022
### Virag Vörös

## Assignment 1.1
"""

!pip install chaospy

import numpy as np
import random
import chaospy as cp
from matplotlib.pyplot import *
import matplotlib as plt 
from tabulate import tabulate

from scipy.integrate import odeint

# to use the odeint function, we need to transform the second order differential equation
# into a system of two linear equations
def model(w, t, p):
	x1, x2 		= w
	c, k, f, w 	= p

	f = [x2, f*np.cos(w*t) - k*x1 - c*x2]

	return f

# discretize the oscillator using the odeint function
def discretize_oscillator_odeint(model, atol, rtol, init_cond, args, t, t_interest):
	sol = odeint(model, init_cond, t, args=(args,), atol=atol, rtol=rtol)

	return sol[t_interest, 0]

if __name__ == '__main__':
    # relative and absolute tolerances for the ode int solver
    atol = 1e-10
    rtol = 1e-10

    # the parameters are no longer deterministic
    c_left      = 0.08
    c_right     = 0.12
    k_left      = 0.03
    k_right     = 0.04
    f_left      = 0.08
    f_right     = 0.12
    y0_left     = 0.45
    y0_right    = 0.55
    y1_left     = -0.05
    y1_right    = 0.05

    # w is deterministic
    w = 1.00

    # create uniform distribution object
    distr_c     = cp.Uniform(c_left, c_right)
    distr_k     = cp.Uniform(k_left, k_right)
    distr_f     = cp.Uniform(f_left, f_right)
    distr_y0    = cp.Uniform(y0_left, y0_right)
    distr_y1    = cp.Uniform(y1_left, y1_right)

    # create the multivariate distribution
    distr_5D = cp.J(distr_c, distr_k, distr_f, distr_y0, distr_y1)

    # quadrature and polynomial degree 1D
    KN = [3, 4]

    # time domain setup
    t_max       = 20.
    dt          = 0.01
    grid_size   = int(t_max/dt) + 1
    t           = np.array([i*dt for i in range(grid_size)])
    t_interest  = int(len(t)/2)

    nr_ODE_evals_full = np.zeros(2)
    nr_ODE_evals_sparse = np.zeros(2)

    for i, quad_poly_deg_1D in enumerate(KN):

      # counter for function evaluations
      ODE_evals_full = 0
      ODE_evals_sparse = 0

      # create the orthogonal polynomials
      P = cp.generate_expansion(quad_poly_deg_1D, distr_5D, normed=True)

      #################### full grid computations #####################
      # get the non-sparse quadrature nodes and weight
      nodes_full, weights_full = cp.generate_quadrature(quad_poly_deg_1D, distr_5D, rule='G', sparse=False)
      # create vector to save the solution
      sol_odeint_full  = np.zeros(len(nodes_full.T))

      # perform full pseudo-spectral approximation
      for j, n in enumerate(nodes_full.T):
          # each n is a vector with 5 components
          # n[0] = c, n[1] = k, c[2] = f, n[4] = y0, n[5] = y1
          init_cond               = n[3], n[4]
          args                    = n[0], n[1], n[2], w
          sol_odeint_full[j]      = discretize_oscillator_odeint(model, atol, rtol, init_cond, args, t, t_interest)
          ODE_evals_full += 1 # track function evaluations

      # obtain the gpc approximation
      sol_gpc_full_approx = cp.fit_quadrature(P, nodes_full, weights_full, sol_odeint_full)

      # compute statistics
      mean_full    = cp.E(sol_gpc_full_approx, distr_5D)
      var_full     = cp.Var(sol_gpc_full_approx, distr_5D)
      ##################################################################

      #################### sparse grid computations #####################
      # get the sparse quadrature nodes and weight
      nodes_sparse, weights_sparse = cp.generate_quadrature(quad_poly_deg_1D, distr_5D, rule='G', sparse=True)
      # create vector to save the solution
      sol_odeint_sparse  = np.zeros(len(nodes_sparse.T))

      # perform sparse pseudo-spectral approximation
      for j, n in enumerate(nodes_sparse.T):
          # each n is a vector with 5 components
          # n[0] = c, n[1] = k, c[2] = f, n[4] = y0, n[5] = y1
          init_cond               = n[3], n[4]
          args                    = n[0], n[1], n[2], w
          sol_odeint_sparse[j]    = discretize_oscillator_odeint(model, atol, rtol, init_cond, args, t, t_interest)
          ODE_evals_sparse += 1 # track function evaluations

      # obtain the gpc approximation
      sol_gpc_sparse_approx = cp.fit_quadrature(P, nodes_sparse, weights_sparse, sol_odeint_sparse)

      # compute Sobol indices
      Sobol_full_first_order = cp.Sens_m(sol_gpc_full_approx, distr_5D)
      Sobol_full_total_order = cp.Sens_t(sol_gpc_full_approx, distr_5D)
      Sobol_sparse_first_order = cp.Sens_m(sol_gpc_sparse_approx, distr_5D)
      Sobol_sparse_total_order = cp.Sens_t(sol_gpc_sparse_approx, distr_5D)

      nr_ODE_evals_full[i] = ODE_evals_full
      nr_ODE_evals_sparse[i] = ODE_evals_sparse

      # plot results 

      index = ['c', 'k', 'f', 'y0', 'y1']

      print('Result for K = N =', quad_poly_deg_1D, ':')

      print('Number of ODE evaluations for full grid = ', nr_ODE_evals_full[i])
      print('Number of ODE evaluations for sparse grid = ', nr_ODE_evals_sparse[i])
      fig, (ax1, ax2) = subplots(2, sharex=True)
      fig.tight_layout()
      x = np.arange(len(index))
      width = 0.35
      ax1.set_title('Sobol indices full grid')
      ax1.bar(x - width/2, Sobol_full_first_order, width=width, label='first order')
      ax1.bar(x + width/2, Sobol_full_total_order, width=width, color='orange', label='total order')
      ax1.legend()
      ax2.set_title('Sobol indices sparse grid')
      ax2.bar(x - width/2, Sobol_sparse_first_order, width=width, label='first order')
      ax2.bar(x + width/2, Sobol_sparse_total_order, width=width, color='orange', label='total order')
      ax2.legend()
      ax2.set_xticks(x)
      ax2.set_xticklabels(index)
      show()

### approach based on Saltelli et al. (Variance based sensitivity analysis of model output. Design and estimator for the total sensitivity index, Andrea Saltelli et. al., Computer Physics Communications, 2010) ###
# Table 2 (b) and (f) chosen as methods for S_i and S_Ti respectively

# generate A and B matrices via Sobol quasi-random sampling

k = 5
N = len(nodes_full.T)  # corresponds to number of full grid nodes for K = N = 4

ODE_evals = 0   # track ODE evaluations

# generate quasi-random Sobol sequence
sobol_samples = distr_5D.sample(2*N, rule="sobol")

A = sobol_samples[:,:N].T
B = sobol_samples[:,N:].T

# S_i and S_Ti calculation

S_i = np.zeros(5)
S_Ti = np.zeros(5)
row_fA = np.zeros(N)

for i in range(k):

  sum_Si = 0
  sum_STi = 0

  for j in range(N):
    # evaluate function for samples out of A and B

    row_fBj = B[j,:]
    init_cond_fBj = row_fBj[3], row_fBj[4]
    args_fBj = row_fBj[0], row_fBj[1], row_fBj[2], w
    fB_j = discretize_oscillator_odeint(model, atol, rtol, init_cond_fBj, args_fBj, t, t_interest)
    ODE_evals += 1 # track function evaluations

    row_fAj = A[j,:]
    init_cond_fAj = row_fAj[3], row_fAj[4]
    args_fAj = row_fAj[0], row_fAj[1], row_fAj[2], w
    fA_j = discretize_oscillator_odeint(model, atol, rtol, init_cond_fAj, args_fAj, t, t_interest)
    ODE_evals += 1 # track function evaluations
    row_fA[j] = fA_j

    # ith column of A_B^i matrix is from B
    if i == 0:
      init_cond_fABij = row_fAj[3], row_fAj[4]
      args_fABij = row_fBj[0], row_fAj[1], row_fAj[2], w
      fABi_j = discretize_oscillator_odeint(model, atol, rtol, init_cond_fABij, args_fABij, t, t_interest)
      ODE_evals += 1 # track function evaluations

    elif i == 1:
      init_cond_fABij = row_fAj[3], row_fAj[4]
      args_fABij = row_fAj[0], row_fBj[1], row_fAj[2], w
      fABi_j = discretize_oscillator_odeint(model, atol, rtol, init_cond_fABij, args_fABij, t, t_interest)
      ODE_evals += 1 # track function evaluations

    elif i == 2:
      init_cond_fABij = row_fAj[3], row_fAj[4]
      args_fABij = row_fAj[0], row_fAj[1], row_fBj[2], w
      fABi_j = discretize_oscillator_odeint(model, atol, rtol, init_cond_fABij, args_fABij, t, t_interest)
      ODE_evals += 1 # track function evaluations
    
    elif i == 3:
      init_cond_fABij = row_fBj[3], row_fAj[4]
      args_fABij = row_fAj[0], row_fAj[1], row_fAj[2], w
      fABi_j = discretize_oscillator_odeint(model, atol, rtol, init_cond_fABij, args_fABij, t, t_interest)
      ODE_evals += 1 # track function evaluations

    elif i == 4:
      init_cond_fABij = row_fAj[3], row_fBj[4]
      args_fABij = row_fAj[0], row_fAj[1], row_fAj[2], w
      fABi_j = discretize_oscillator_odeint(model, atol, rtol, init_cond_fABij, args_fABij, t, t_interest)
      ODE_evals += 1 # track function evaluations

    sum_Si += fB_j * (fABi_j - fA_j)
    sum_STi += (fA_j - fABi_j)*(fA_j - fABi_j)
  
  S_i[i] = (1/N) * sum_Si / np.var(row_fA)
  S_Ti[i] = (1/(2*N)) * sum_STi / np.var(row_fA)

# plot results
print('Number of ODE evaluations = ', ODE_evals)

fig, ax = subplots()
x = np.arange(len(index))
width = 0.35
ax.bar(x - width/2, S_i, width=width, label='first order')
ax.bar(x + width/2, S_Ti, width=width, color='orange', label='total order')
ax.set_title('Sobol indices Saltelli et al.')
ax.set_xticks(x)
ax.set_xticklabels(index)
ax.legend()
show()

"""## Assignment 2.2

"""

if __name__ == '__main__':
    N = 40
    l = 1

    # mesh size in both directions (NxN)
    mesh_size_x = N
    mesh_size_y = N

    # compute the coordinates that we are interested in are in the middle of each Cartesian cell
    x_coord = np.arange(1./(2*mesh_size_x), 1., 1./mesh_size_x)
    y_coord = np.arange(1./(2*mesh_size_y), 1., 1./mesh_size_y)

    mesh_coord = {}
    for i in range(mesh_size_x):
        for j in range(mesh_size_y):
            mesh_coord[i*mesh_size_x + j] = x_coord[i], y_coord[j]

    # define the given mean function
    mean_func = 0.1

    # define the given covariance functions
    # the exponential covariance function
    norm_1 = lambda x, y: np.sqrt((x[0]-y[0])**2+(x[1]-y[1])**2)
    cov_1 = lambda x, y: np.exp(-(norm_1(x, y)) / (l))
    # the squared exponential function
    norm_2 = lambda x, y: (x[0]-y[0])**2+(x[1]-y[1])**2
    cov_2 = lambda x, y: np.exp(-(norm_2(x, y))/(2*l**2))

    # create data structures to hold the covariance matrices
    no_rows_x = mesh_size_x*mesh_size_x
    no_rows_y = mesh_size_y*mesh_size_y
    covhat_1 = np.zeros((no_rows_x, no_rows_y))
    covhat_2 = np.zeros((no_rows_x, no_rows_y))
    
    # create the discrete mean function, i.e. create mean vector
    mean_hat = mean_func*np.ones(no_rows_x)
    
    # create the discrete covariance function, i.e. create covariance matrix
    for i in range(no_rows_x):
    	for j in range(no_rows_y):
            covhat_1[i, j] = cov_1(mesh_coord[i], mesh_coord[j])
            covhat_2[i, j] = cov_2(mesh_coord[i], mesh_coord[j]) 
            if i==j:
                covhat_2[i, j] += 0.000001

    # perform a Cholesky decomposition of the covariance matrix
    E_cov_1 = np.linalg.cholesky(covhat_1)
    E_cov_2 = np.linalg.cholesky(covhat_2)
    
    # we need the standard mean and variance to sample for a standard multivariate Gaussian
    standard_mean = np.zeros(no_rows_x)
    standard_cov = np.identity(no_rows_x)
    
    # plot the sample for cov_1
    no_figures = 3
    for i in range(no_figures):
        figure()
        # sample from the given random field
        random_sample = np.array(np.random.multivariate_normal(standard_mean, standard_cov, 1)).T
        rotated_sample = mean_hat + np.dot(E_cov_1, random_sample).T[0]
        transformed_sample = rotated_sample.reshape(mesh_size_x, mesh_size_y)
        # plot the results
        title("Exponential Covariance: Sample " + str(i+1))
        imshow(transformed_sample, cmap='coolwarm')

    # plot the sample for cov_2
    for i in range(no_figures):
        figure()
        # sample from the given random field
        random_sample = np.array(np.random.multivariate_normal(standard_mean, standard_cov, 1)).T
        rotated_sample = mean_hat + np.dot(E_cov_2, random_sample).T[0]
        tansformed_sample = rotated_sample.reshape(mesh_size_x, mesh_size_y)
        # plot the results
        title("Squared Exponential Covariance: Sample " + str(i+1))
        imshow(transformed_sample, cmap='coolwarm')
    show()

"""## Assignment 3.1"""

def eigenvalues(M):                                 # Fuctions to compute eigenvalues 
    eigenv=np.zeros(M)
    for i in range(M):
        eigenv[i]=1./pow((i-0.5)*np.pi,2)
    return eigenv


def Weiner_process_def(zeta, t,dt):                 # Fucntion to compute Weiner process using def 
    n = len(t)
    W = np.zeros(n)
    temp_W = np.zeros(n)

    for i in range(1, n):
        temp_W[i] = np.sqrt(dt)*zeta[i - 1]
        W[i] = W[i - 1] + temp_W[i]
    
    return W


def Weiner_process_KL(zeta, t, dt, M):              # function to compute Weiner process using KL apporx 
    n = len(t)
    W = np.zeros(n)
    for i in range(1, M):
        W += np.sqrt(2)*zeta[i - 1]*np.sin((i + 0.5)*np.pi*t)/((i + 0.5)*np.pi)

    return W

if __name__ == '__main__':

    N = 1000
    M = [10, 100, 1000]

    dt = 1./N                # time discretization
    t = np.arange(0, 1+dt, dt)

    # generate random variables
    zeta = np.random.normal(0, 1, N)

    figure()                                        # plot of eigenvalues 
    eigenv=eigenvalues(N)   
    plot(eigenv)
    xlabel('M values', fontsize=10)
    ylabel('eigen values', fontsize=10)
    title('Eigenvalues plot', fontsize=15)
    
    
    figure()                                            #plot of Weiner prpcess using def 
    W_def = Weiner_process_def(zeta, t, dt)
    plot(t, W_def)
    xlabel('time', fontsize=10)
    ylabel('Values', fontsize=10)
    title('Wiener process generated from definition with N=1000', fontsize=15)
    show()
    
    figure()
    for m in M:                                              # plot of Weiner process using KL approx 
        W_KL = Weiner_process_KL(zeta, t,dt, m)
        plot(t, W_KL, label='KL approx with M = ' + str(m))
        xlabel('time', fontsize=10)
        ylabel('Values', fontsize=10)
        title('Wiener process approximated from 3 dicretisations of KL expension', fontsize=15)

    legend(loc='best', fontsize=10)

"""We can notice that the eigenvalues quickly converge to zero with high n number.

M corresponds to the number of KL terms

N corresponds to the number of samples in the Weiner definition. 

The more we increase the number of samples, the more get closer the definition

## Assignment 3.2
"""

def discretize_oscillator_sys(t, dt, params, f):
    c, k, w, y0, y1 = params
    z = np.zeros(len(t))
    temp_z = np.zeros(len(t))
    z[0] = y0
    temp_z[0] = y1
    for i in range(0, len(t) - 1):
        temp_z[i + 1] = temp_z[i] + dt*(-k*z[i] - c*temp_z[i]+f[i]*np.cos(w*t[i]))
        z[i + 1] = z[i] + dt*temp_z[i]

    return z

def Weiner_process_def(t, f_mean):
    n = len(t)
    W = np.zeros(n)
    temp_W = np.zeros(n)
    W[0] = f_mean

    for i in range(1, n):
        temp_W[i] = np.sqrt(t[i]-t[i-1])*np.random.randn()
        W[i] = W[i - 1] + temp_W[i]

    return W


def Weiner_process_KL(t, f_mean, KL_dim):
    n = len(t)
    W = np.zeros(n) + f_mean
    zeta = np.random.normal(0, 1, KL_dim)

    for i in range(KL_dim):
        W += np.sqrt(2./t[-1])*zeta[i]*(t[-1]**2)*np.sin(((i + 1 + 0.5)*np.pi*t)/t[-1])/((i + 1 + 0.5)*np.pi)

    return W


if __name__ == '__main__':

  atol = 1e-10
  rtol = 1e-10


  c   = 0.5
  k   = 2.0
  y0  = 0.5
  y1  = 0.
  w  = 1.0
  f_mean = 0.5

  t_max       = 10.
  dt          = 0.01
  grid_size   = int(t_max/dt) + 1
  t           = np.array([i*dt for i in range(grid_size)])
  t_interest  = -1
  n_timesteps = grid_size

  N = [10,100,1000]

  M = [5, 10, 100]

  

  table_def=np.zeros((len(N),3))
  table_KL=np.zeros((len(N),3))
  table_def[:,0]=N
  table_KL[:,0]=M
  figure()
  for k,n in enumerate(N): 
    ode_weiner_def = np.zeros( (n, n_timesteps))

    for i in range(n):
          W_def = Weiner_process_def(t, f_mean)
          params   = c, k, w, y0, y1
          f = W_def
          v=discretize_oscillator_sys(t, dt, params, f)
          ode_weiner_def[i, :]  = v
          plot(t, ode_weiner_def[i, :])
          title('Uncertainty propogartion using Weiner process definition ', fontsize=15)

    mean_mc = np.mean(ode_weiner_def, axis=0)
    var_mc = np.var(ode_weiner_def, axis=0, ddof=1)
    table_def[k,1]=mean_mc[t_interest]
    table_def[k,2]=var_mc[t_interest]
  
  head_mc= ["N Values", "Mean", "Variance"]
  print(" Table of mean and variance of y(10) for Weiner process definition for different values of N")
  print(tabulate(table_def, headers=head_mc, tablefmt="grid"))
  show()

  for j in range(0, len(M)):
          ode_weiner_kl = np.zeros( (1000, n_timesteps))
          figure()
          for i in range(1000):
                  W_KL= Weiner_process_KL(t, f_mean, M[j])
                  params   = c, k, w, y0, y1
                  f= W_KL
                  v= discretize_oscillator_sys(t, dt, params, f)
                  ode_weiner_kl[i, :] =v 
                  plot(t, ode_weiner_kl[i, :])
                  title('Uncertainty propogartion using Weiner process KL expension with M =' +str(M[j]), fontsize=15)

          mean_kl = np.mean(ode_weiner_kl, axis=0)
          var_kl = np.var(ode_weiner_kl, axis=0, ddof=1)
          table_KL[j,1]=mean_kl[t_interest]
          table_KL[j,2]=var_kl[t_interest]
          


  head_kl= ["M Values", "Mean", "Variance"]


  print(" Table of mean and variance of y(10) for Weiner process using KL approximation for N= 1000 and different values of M")
  print(tabulate(table_KL, headers=head_kl, tablefmt="grid"))

  show()