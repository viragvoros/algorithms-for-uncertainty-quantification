# -*- coding: utf-8 -*-
"""Programming1_Virag_Vörös.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/102cRYOXQ88Ki53q7cnZh6Te4KawRk9di

# Algorithms for Uncertainty Quantification
## Programming assignment 1, Summer term 2022
### Virag Vörös

Assignment 1.1
"""

!pip install chaospy

import numpy as np
import random
import chaospy as cp
from matplotlib.pyplot import *

g = np.array([1.3, 1.7, 1.0, 2.0, 1.3, 1.7, 2.0, 2.3, 2.0, 1.7, 1.3, 1.0, 2.0, 1.7, 1.7, 1.3, 2.0])

# our computed mean and variance 
our_mean=sum(g)/len(g)
our_var=sum((g-our_mean)**2)/(len(g)-1)

# numpy mean and variance
mean=np.mean(g)
var=np.var(g)                     # ddof =1 for an unbiased estimator ( source: https://numpy.org/doc/stable/reference/generated/numpy.var.html)

# printing
print('Our calculated mean is: ', our_mean, ' and the numpy mean is: ', mean)
print( 'Our calculated variance is: ', our_var, ' and the numpy variance is: ', var)
print('The difference between the mean values is: ', abs(our_mean - mean), ' and the difference between the variance values is: ', abs(our_var - var))

"""The mean calculated by the method implemented by us is the same as the mean from numpy. However, the variance differs by a minor value. This is due to numpy's default variance calculation, which considers biased estimators and divides by N. Changing the delta degrees of freedom variable ddof to 1 results in a division by (N-1), corresponding to an unbiased estimator.

Assignment 2.1
"""

if __name__ == '__main__':
    # declare function to integrate via Monte Carlo sampling
    func = lambda x: np.sin(x)

    # declare vector with number of samples
    N = [10, 100, 1000, 10000, 100000, 1000000, 10000000, 100000000]

    I_hat       = np.zeros(len(N))                                                # approximate Monte Carlo integral result
    I           = 1 - np.cos(1)                                                   # exact solution (integral)
    est_std_dev = np.zeros(len(N))                                                # Monte Carlo standard error (RMSE)
    rms = np.zeros(len(N))                                                        # exact error (epsilon)

    distribution = cp.Uniform(0, 1)
    for i, n in enumerate(N):
        samples=distribution.sample(size=n)                                       # samples from chaospy      
        I_hat[i]=np.mean(func(samples))                                           # calculation I_hat using mean 
        est_std_dev[i] =np.sqrt(np.var((func(samples) - I_hat[i]), ddof=1))/np.sqrt(n) # calculation the rmse using var  (ddof=1 to have an unbiased estimator) 
        rms[i] =np.abs(I_hat[i] - I)                                              # exact error 
        
    loglog(N, est_std_dev, 'r', label='RMSE')
    loglog(N, rms, 'b', label=r'$\epsilon$')
    legend(loc='best')
    xlabel('no. of samples')
    show()

"""In case we run the simulation multiple times, the standard error of the Monte Carlo sampling remains more or less the same (decreases monoton w.r.t the number of samples). However, the exact error (epsilon) differs in every sampling, while following more or less the same tendency as the Monte Carlo sampling.
This could happen due to the formula of the RMSE, as it uses variances (compared at multiple samples), while in case of the exact error we compare only the exact values and the mean.

Assignment 2.2
"""

if __name__ == '__main__':
    # declare function to integrate via Monte Carlo sampling
    func = lambda x: np.sin(x)

    # declare vector with number of samples
    N = [10, 100, 1000, 10000, 100000, 1000000, 10000000, 100000000]

    I_hat       = np.zeros(len(N))                                                # approximate Monte Carlo integral result
    I           = np.cos(2) - np.cos(4)                                           # exact solution 
    est_std_dev = np.zeros(len(N))                                                # Monte Carlo standard error (RMSE)
    rms = np.zeros(len(N))                                                        # exact error (epsilon)
    
    a=2
    b=4
    
    transformation= False                                                         # true or false if we want to make a transformation or not 

    for i, n in enumerate(N):
        if transformation== True:                                                 # sampling from (0,1), then transforming to (2,4)
            distribution = cp.Uniform(0, 1)
            temp=distribution.sample(size=n)
            samples =a +(b-a)*temp 
   
        else:                                                                     # sampling directly from (2,4)
            distribution = cp.Uniform(a, b)
            samples=distribution.sample(size=n)                                   # samples from chaospy
            
        I_hat[i]= (b-a) * np.mean(func(samples))
        est_std_dev[i] =np.sqrt(np.var((func(samples) - I_hat[i]), ddof=1))/np.sqrt(n)   # ddof=1 to have an unbiased estimator 
        rms[i] =np.abs(I_hat[i] - I)
        
    loglog(N, est_std_dev, 'r', label='RMSE')
    loglog(N, rms, 'b', label=r'$\epsilon$')
    legend(loc='best')
    xlabel('no. of samples')
    show()

"""In assignment 2.2, very similar results can be seen as in assignment 2.3. To obtain these results, the entire formula of the approximate Monte Carlo integral had to be used as the following: I_hat = **(b-a)***np.mean(f(samples), which takes the length of the sampled interval into consideration as well.

"""



"""Assignment 3.1"""

import numpy as np
import matplotlib.pyplot as plt

N = [10, 100, 1000, 10000]
mean = np.array([-0.4, 1.1])
V = np.array([[1.0, 0.4], [0.4, 1.0]])                                            # covariance 

mc_v = np.zeros(V.shape)                                                          # define the Monte Carlo covariance and mean vectors 
mc_mean= np.zeros(mean.shape)

error_mean=np.zeros((len(N), mean.shape[0]))
rmse_mean=np.zeros((len(N), mean.shape[0]))

error_v=np.zeros((len(N), V.shape[0], V.shape[1]))

for i, n in enumerate(N): 
  samples = np.random.multivariate_normal(mean, V, n).T 
  for j in range(len(mean)):                                                      # calculating mean
    mc_mean[j]=np.mean(samples[j,:])
    error_mean[i,j]=np.abs(mc_mean[j] - mean[j])
    rmse_mean[i,j]=np.sqrt(np.var(samples[j,:])/(n-1))
    for k in range(len(mean)):
        mc_v[j, k] = 0                                                            # clearing mc_v before each n
        for nb in range(n):
              mc_v[j, k] += (samples[j, nb] - mc_mean[j]) * (samples[k, nb] - mc_mean[k])/(n-1)
        error_v[i,j,k]=np.abs(mc_v[j,k]-V[j,k])

plt.loglog(N, error_mean[:, 0], 'r', label=r'absolute1')
plt.loglog(N, error_mean[:, 1], 'b', label=r'absolute2')
legend(loc='best')
plt.ylabel('mean absolute error ')
plt.xlabel('no. of samples')

plt.figure()
plt.loglog(N, rmse_mean[:, 0], 'r', label=r'rmse1')
plt.loglog(N, rmse_mean[:, 1], 'b', label=r'rmse2')
legend(loc='best')
plt.ylabel('rmse')
plt.xlabel('no. of samples')

plt.figure()
plt.plot(N, error_v[:, 0, 0], 'r', label=r'diag')
plt.plot(N, error_v[:, 0, 1], 'b', label=r'off-diag')
legend(loc='best')
plt.xscale("log")
plt.ylabel('covariance absolute error')
plt.xlabel('no. of samples')

plt.show()

"""The RMSE decreases w.r.t increasing number of samples, the absolut error of the mean follows the same tendency. As an unbiased estimator is used to calculate the covariance, with increasing number of samples, the covariance also decreases and converges to 0.

Assignment 3.2
"""

import numpy as np
import chaospy as cp
from scipy.integrate import odeint
import matplotlib.pyplot as plt

# to use the odeint function, we need to transform the second order differential equation into a system of two linear equations
def model(w, t, p):
	x1, x2 		= w
	c, k, f, w 	= p

	f = [x2, f*np.cos(w*t) - k*x1 - c*x2]

	return f

# discretize the oscillator using the odeint function
def discretize_oscillator_odeint(model, atol, rtol, init_cond, args, t):
	sol = odeint(model, init_cond, t, args=(args,), atol=atol, rtol=rtol)

	return sol[:, 0]

if __name__ == '__main__':
    # relative and absolute tolerances for the ode int solver
    atol = 1e-10
    rtol = 1e-10

    c = 0.5
    k = 2.0
    f = 0.5
    w = 1.0
    y0 = 0.5
    y1 = 0.

    N_ref = 1000000
    N = [10, 100, 1000, 10000]

    # specify time domain setup
    t_max       = 20.
    dt          = 0.01
    grid_size   = int(t_max/dt) + 1
    t           = np.array([i*dt for i in range(grid_size)])
    t_comp      = int(len(t)/2)

    # initial conditions and parameters setup
    init_cond = y0, y1
    args = c, k, f, w

    # perform deterministic computations

    det_solution_odeint = discretize_oscillator_odeint(model, atol, rtol, init_cond, args, t)

    print('y(10) = ', det_solution_odeint[t_comp])  # deterministic result

    exp = np.zeros(len(N))  # array for expectation values
    var = np.zeros(len(N))  # array for variance values

    ref_exp = -0.43893703   # reference values (N = 1000000)
    ref_var = 0.00019678

    rel_error_exp = np.zeros(len(N))  # array for rel. error of exp. value
    rel_error_var = np.zeros(len(N))  # array for rel. error of variance

    # sample w
    distribution_w = cp.Uniform(0.95, 1.05)

    for i, n in enumerate(N):
      samples = distribution_w.sample(size=n)
      trajectory_odeint = np.zeros(n)

      for j, w in enumerate(samples):
        args = c, k, f, w
        trajectory_odeint[j] = discretize_oscillator_odeint(model, atol, rtol, init_cond, args, t)[t_comp]

      exp[i] = np.mean(trajectory_odeint)
      var[i] = np.var(trajectory_odeint, ddof=1)

      rel_error_exp[i] = np.abs(1 - (exp[i]/ref_exp))
      rel_error_var[i] = np.abs(1 - (var[i]/ref_var))

    print('Mean of y(10) = ', exp)
    print('Variance of y(10) = ', var)

    plt.figure()
    plt.loglog(N, rel_error_exp, 'r', label='rel. error of mean')
    plt.loglog(N, rel_error_var, 'b', label='rel. error of variance')
    plt.legend(loc='best')
    plt.xlabel('no. of samples')
    plt.show()

    # sample 5 values for 5 trajectories
    sample_w = distribution_w.sample(size=5) 
    trajectory_sample = np.zeros((5,len(t)))
   
    plt.figure()
    plt.xlabel('t')
    plt.ylabel('y(t)')
    plt.title('5 sampled trajectories for different w')
    for i,w in enumerate(sample_w):
      args = c, k, f, w
      trajectory_sample[i,:] = discretize_oscillator_odeint(model, atol, rtol, init_cond, args, t)
      plt.plot(t, trajectory_sample[i,:])
    plt.show()

"""The relative error of the mean and the variance decreases w.r.t increasing number of samples as we expected.

Assignment 4.1
"""

import numpy as np
from scipy.stats import beta
from matplotlib.pyplot import *

# function to compute relative error
def get_rel_err(approx, ref): 
    return abs(1. - approx/ref)

# standard Monte Carlo sampling
def std_mcs(func, n_samples):
    samples = np.random.uniform(0, 1, n_samples)
    I = np.mean(func(samples))

    return I 

# control variate: we assume that we know the integral 'integral_cv_eval' of the control variate
def control_variate(func, cv, integral_cv_eval, n_samples): 
    samples = np.random.uniform(0, 1, n_samples) 
    approx_int  = np.mean(func(samples) - cv(samples))

    I  = integral_cv_eval + approx_int

    return I 
    
# importance sampling using a beta distribution
def importance_sampling(func, a, b, n_samples):
    samples = np.random.beta(a, b, size=n_samples)

    k = lambda x: func(x)/beta.pdf(x, a, b)   # ration f and g 

    I = np.mean(k(samples))

    return I

if __name__ == '__main__':
    # declare function to integrate via Monte Carlo sampling
    func = lambda x: np.exp(x)

    # compute integral_0^1 func(x)dx
    sol = func(1.)-func(0.)

    # declare vector with number of samples
    N = [1000, 10000, 100000, 1000000]

    # declare the control variates
    control_var= [lambda x: x, lambda x: 1. + x]
    
    # compute their integral
    integral_cv     = [0.5, 1.5]

    # declare values for the a and b parameters for the beta distributions
    a = [5, 0.5]
    b = [1, 0.5]

    # vector to put all relative errors
    rel_err_mcs   = np.zeros(len(N))
    rel_err_cv    = np.zeros((len(N), len(integral_cv)))
    rel_err_ip    = np.zeros((len(N), len(a)))

    # for each N, perform Monte Carlo integration
    for i, n in enumerate(N):
        mcs= std_mcs(func, n)                                                     # Monte Carlo sampling 
        rel_err_mcs[i]  = get_rel_err(mcs,sol) 

        for j in range(len(a)):
            cv  = control_variate(func, control_var[j], integral_cv[j], n)        # control variate 
            ip  = importance_sampling(func, a[j], b[j], n)                        # importance sampling 

            rel_err_cv[i, j] = get_rel_err(cv, sol)                               # computing error
            rel_err_ip[i, j] = get_rel_err(ip, sol)                               # computing error

    loglog(N, rel_err_mcs, 'r', label='relative error mcs')
    loglog(N, rel_err_cv[:, 0], 'g', label='relative error when control variate = x')
    loglog(N, rel_err_cv[:, 1], 'b', label='relative error when control variate = 1 + x')
    loglog(N, rel_err_ip[:, 0], 'c', label='relative error importance sampling, a = 5, b = 1')
    loglog(N, rel_err_ip[:, 1], 'm', label='relative error importance sampling, a = 0.5, b = 0.5')
    plt.rcParams["figure.figsize"] = (15,5)

    legend(loc='best')
    show()

"""Assignment 4.2"""

import numpy as np
import chaospy as cp
from scipy.integrate import odeint
import matplotlib.pyplot as plt

# to use the odeint function, we need to transform the second order differential equation into a system of two linear equations
def model(w, t, p):
	x1, x2 		= w
	c, k, f, w 	= p

	f = [x2, f*np.cos(w*t) - k*x1 - c*x2]

	return f

# discretize the oscillator using the odeint function
def discretize_oscillator_odeint(model, atol, rtol, init_cond, args, t, t_interest=None):
    sol = odeint(model, init_cond, t, args=(args,), atol=atol, rtol=rtol)

    if t_interest == None:
       return sol[:, 0]

    return sol[t_interest, 0]

if __name__ == '__main__':
    # relative and absolute tolerances for the ode int solver
    atol = 1e-10
    rtol = 1e-10

    # initiate deterministic parameters setup as specified in the assignement [c, k, f, y0, y1]
    c = 0.5
    k = 2.0
    f = 0.5
    w = 1.0
    y0 = 0.5
    y1 = 0.

    N_ref = 1000000
    N = [10, 100, 1000, 10000]

    # specify time domain setup
    t_max       = 20.
    dt          = 0.01
    grid_size   = int(t_max/dt) + 1
    t           = np.array([i*dt for i in range(grid_size)])
    t_comp      = int(len(t)/2)

    # initial conditions and parameters setup
    init_cond = y0, y1
    args = c, k, f, w

    # Monte Carlo sampling
    exp_MC = np.zeros(len(N))                                                     # array for expectation values of Monte Carlo sampling
    var_MC = np.zeros(len(N))                                                     # array for variance values of Monte Carlo sampling

    exp_QMC = np.zeros(len(N))                                                    # array for expectation values Quasi Monte Carlo sampling
    var_QMC = np.zeros(len(N))                                                    # array for variance values Quasi Monte Carlo sampling

    ref_exp = -0.43893703                                                         # reference mean (N = 1000000)
    ref_var = 0.00019678                                                          # reference variance (N = 1000000)

    rel_error_exp_MC = np.zeros(len(N))                                           # array for rel. error of exp. value of Monte Carlo sampling
    rel_error_var_MC = np.zeros(len(N))                                           # array for rel. error of variance of Monte Carlo sampling

    rel_error_exp_QMC = np.zeros(len(N))                                          # array for rel. error of exp. value of Quasi Monte Carlo sampling
    rel_error_var_QMC = np.zeros(len(N))                                          # array for rel. error of variance of Quasi Monte Carlo sampling

    # sample w with Monte Carlo and Quasi Monte Carlo
    distribution_w = cp.Uniform(0.95, 1.05)

    for i, n in enumerate(N):
      samples_MC = distribution_w.sample(size=n)
      trajectory_odeint_MC = np.zeros(n)

      samples_QMC = distribution_w.sample(size=n, rule='H')
      trajectory_odeint_QMC = np.zeros(n)

      # MC evaluation
      for j, w in enumerate(samples_MC):
        args = c, k, f, w
        trajectory_odeint_MC[j] = discretize_oscillator_odeint(model, atol, rtol, init_cond, args, t)[t_comp]

      # QMC evaluation
      for j, w in enumerate(samples_QMC):
        args = c, k, f, w
        trajectory_odeint_QMC[j] = discretize_oscillator_odeint(model, atol, rtol, init_cond, args, t)[t_comp]

      exp_MC[i] = np.mean(trajectory_odeint_MC)
      var_MC[i] = np.var(trajectory_odeint_MC, ddof=1)

      exp_QMC[i] = np.mean(trajectory_odeint_QMC)
      var_QMC[i] = np.var(trajectory_odeint_QMC, ddof=1)

      rel_error_exp_MC[i] = np.abs(1 - (exp_MC[i]/ref_exp))
      rel_error_var_MC[i] = np.abs(1 - (var_MC[i]/ref_var))

      rel_error_exp_QMC[i] = np.abs(1 - (exp_QMC[i]/ref_exp))
      rel_error_var_QMC[i] = np.abs(1 - (var_QMC[i]/ref_var))

    # print results
    print('Mean of Monte Carlo sampled y(10) = ', exp_MC)
    print('Variance of Monte Carlo sampled y(10) = ', var_MC)

    print('Mean of Quasi Monte Carlo sampled y(10) = ', exp_QMC)
    print('Variance of Quasi Monte Carlo sampled y(10) = ', var_QMC)

    plt.figure()
    plt.loglog(N, rel_error_exp_MC, 'r', label='rel. error of MC mean')
    plt.loglog(N, rel_error_exp_QMC, 'b', label='rel. error of QMC mean')
    plt.legend(loc='best')
    plt.xlabel('no. of samples')
    plt.show()

    plt.figure()
    plt.loglog(N, rel_error_var_MC, 'r', label='rel. error of MC variance')
    plt.loglog(N, rel_error_var_QMC, 'b', label='rel. error of QMC variance')
    plt.legend(loc='best')
    plt.xlabel('no. of samples')
    plt.show()

